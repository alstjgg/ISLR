# 2.1 What Is Statistical Learning

---

**Statistical learning** refers to a set of approaches for estimating $f$ in

$$
Y = f(X) + ϵ,
$$

where

- $Y$ : a quantitive response
- $X=X_1, X_2, X_3, ..., X_p$ : *p* different predictors
- *ϵ* : a random error term independent of $X$

# 2.1.1 Why Estimate $f$?

There are two main reasons for estimating $f$.

- Prediction
- Inference

## Prediction

Given a set of input $X$, we want to predict the unkown output $Y$.

We can predict $Y$ by using

$$
\hat Y = \hat f(X),
$$

where

- $\hat f$ : estimate for $f$
- $\hat Y$ : resulting prediction for $Y$

In prediction, we are not interested in the exact form of $\hat f$, as long as it yields accurate predictions for $Y$, and is treated as a *black box*.

The **accuracy** depends on 2 quantities.

- reducible error
- irreducible error

**Reducible error** occurs because $\hat f$ will not be a perfect estimate for $f$. It *can be reduced* because we can improve accuracy by using more appropriate statistical learning techniques for estimating $f$.

**Irreucible error** *cannot be reduced* because $\hat Y$ is a function of $\epsilon$, which cannot be predicted using $X$. $\epsilon$ may contain **unmeasured variables** and **unmerasurable variables**.

Thus, given an estimate $\hat f$ and a set of predictors $X$ which uields the prediction $\hat Y$,

$$
E(Y-\hat Y)^2=E|f(x)+\epsilon-\hat f(X)|^2\\=|f(X)-\hat f(X)|^2+Var(\epsilon),
$$

where

- $E(Y-\hat Y)^2$ : expected alue of the squared difference between the predicted and actual value of $Y$
- $|f(X)-\hat f(X)|^2$ : reducible error
- $Var(\epsilon)$ : irreudcible error, variance associated with the error term $\epsilon$

The aim of **prediction** is to *minimize the reducible error*.

## Inference

Inference is the question of understanding the association between $Y$ and $X$. Therefore, $\hat f$ cannot be treated as a black box because we need to know its exact form.

Questions regarding inference can include...

- Which predictors are associated with the response?
- What is the relationship between the response and each predictors?
- Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, ot is the relationship more complicated?

Depending on whether the goal is prediction, inference, or a combination of the two, different methods for estimating $f$ can be used. Different models offer different amount of **accuracy** and **interpretability**, and there is always a tradeoff between them.

# 2.1.2 How Do We Estimate $f$?

Our goal is to apply a statistical learning method to find a function $\hat f$ such that $Y\approx \hat f(X)$ for any observation $(X, Y)$. These methods can be characterezed into ...

- Parametric methods
- Non-Parametric methods

## Parametric methods

Parametric methods involve a 2 step model based approach

1. **Assume a form for $f$(a model)**
    
    For example, assume $f$ is linear to $X$.
    
    $$
    f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
    $$
    
2. ***Train*, or *fit*, the model using the training data**
    
    We try to find parameters $\beta_0, \beta_1, ..., \beta_p$ such that
    
    $$
    Y\approx\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
    $$
    

Thus, a parametric methods reduces the problem of estimating $f$ to estimating a set of parameters. This makes it **easier**.

However, the model will **not match the true $f$**.

We can address this problem by choosing flexible models that can fit different possible forms for $f$, but complex models can lead to overfitting.

## Non-Parametric methods

Non-parametric methods do not make assumptions about the form of $f$, but rather seek an estimate for $f$ that gets as close as possible to the data points.

Although these methods have the potential to accurately fir a wider range of possible shapes for $f$, this requires a large number of observations.

# 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability

There is a tradeoff between model flexibility and interpretability.

![https://marcfbellemare.com/wordpress/wp-content/uploads/2019/01/JamesFigure.jpg](https://marcfbellemare.com/wordpress/wp-content/uploads/2019/01/JamesFigure.jpg)

Since restrictive models are more interpretable, they are better for **inference** problems.

Flexible models offer more accuracy for **prediction** problems, though one must deal with the potential of overfitting when using highly flexible models.

# 2.1.4 Supervised Versus Unsupervised Learning

Statistical learning problems fall into one of 2 categories

- **Supervised learning**
    
    For each observation $x_i$, there is an associated response measurement $y_i$.
    
- **Unsupervised learning**
    
    For every observation, we observe a vector of measurements $x_i$, but no associated response $y_i$. We seek to understand the relationships between variables or observations.
    
    *→ example: cluster analysis*
    

# 2.1.5 Regression Versus Classification Problems

Variables can be characterized into 2 categories.

- **Quantitative**
    
    → take on numerical values
    
    **→ regression problems**
    
- **Qualitative**(Categorical)
    
    → take on values in one of K different *classes*, or categories
    
    **→ classification problems**