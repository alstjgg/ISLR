# 2.1 Statistical Learning?

Input Variables: X_1, X_2, ...
Output Variables: Y
=>통계적으로 관련성 파악
=>수식으로 표현 => Y = f(X) + e (i.e. 종속변수가 독립변수에 의해 표현된다)

Statistical Learning => f 함수를 예측하는 것

## 2.1.1 Why Estimate f?

#### 예측
- 대부분 X는 주어지지만 Y는 주어지지 않음
- Y 햇 = f햇(X) 을 통해 예측 (오차 평균이 0)

f햇은 f와 같지 않고 오차가 있다 => Reducible Error (f햇을 개선하면 오차 줄일 수 있음)
f(X)으로 Y햇을 예측한다 => Irreducible Error

Example: 집단 A의 Input이 X, Output이 Y고, 집단 B의 Input이 X햇, Output이 Y햇이면, X로 Y를 예측하는 것은 f, X햇으로 Y햇을 예측하는 것은 f햇이다.
이때, f햇을 개선하여 f처럼 오차를 줄이는 것은 가능하나, A집단과 B집단 자체가 다르기 때문에 (Y햇과 Y) 줄일 수 없는 오차는 잔존한다.

Eq 2.3 : E(Y-Y햇)^2 = E[f(X) + e - f햇(X)]^2 = [f(X) - f햇(X)]^2 + var(e)

#### 추측
예측하지 못하고 추측이 필요한 경우
- 어느 예측이 응답과 관련있는지
- 응답과 각 에측사이의 관계
- Y와 예측이 선형으로 나타낼 수 있는지

결론: 모델에 따라 예측, 추측 모두 필요할 수도 있다.

## 2.1.2 How Do We Estimate f?

1. Parametric Methods
  - 함수 형태에 대한 가정을 한다
  - 모델을 선택한 후 training data로 학습시켜서 매개변수를 찾는다
2. Non-Parametric Methods
  - 함수 형태에 대해 명시적인 가정을 하지 않는다
  - 너무 대강하지 않되, 예측치와 최대한 가깝게 지나는 f를 예측한다

## 2.1.3 The Trade-Off between Prediction Accuracy and Model Interpretability

모델의 해석가능성과 정확도 반비례한다

Example

Low Interpretability                                      High Interpretability
Deep Learning - SVM - Bagging, Boosting - Generalized Addictive Models Trees - Least Squares - Subset Selection Lasso
High Flexibility                                          Low Flexibility

High Interpretability 선택 이유
- 추측하는 경우

High Flexibility  선택 이유
- (복잡한) 예측하는 경우

## 2.1.4 Supvervised Vs Unsupervised Learning

Supervised Learning
- 


  
  
  
